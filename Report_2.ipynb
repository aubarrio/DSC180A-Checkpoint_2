{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN-LPA Report\n",
    "The GCN-LPA approach aims to unify node feature propagation along with edge propagation. This means that we can learn the weights of the edges of a network by assessing how a feature of a node relates to the features of its neighbors and how this feature influences those features that are on the other side of the edges that connect to that node.\n",
    "\n",
    "So rather than looking at different layers of neighbors like in graphSage, here we analyze the importance of a neighbor through edge weights. The goal is to have edge weights that are learnable so that we can improve our classification accuracy. \n",
    "\n",
    "The LPA propagates labels of known nodes to those nodes that have not been labeled and through this finds communities within a network. Once we minimize the loss of the predictions reached through the LPA we reach and learn the optimal edge weights of the network. Since now we believe to have the optimal edge weights we feed them into a GCN which will learn and classify node labels. \n",
    "\n",
    "Here we can see how the LPA helps the GCN by acting as a regularizer which leads to better results when compared to just a GCN.  This further shows how the GCN-LPA approach differs from the graphsage approach, rather than using aggregated information through a fully connected network we use an optimized graph with optimized edge weights into a fully connected network that will then learn over our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN-LPA Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLPA(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, adj, dropout_rate):\n",
    "        super(GCNLPA, self).__init__()\n",
    "\n",
    "        self.gc1 = GCNLPAConv(nfeat, nhid, adj)\n",
    "        self.gc2 = GCNLPAConv(nhid, nclass, adj)\n",
    "        self.dropout = dropout_rate\n",
    "\n",
    "    def forward(self, x, adj, y):\n",
    "        x, y_hat = self.gc1(x, adj, y)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x, y_hat = self.gc2(x, adj, y_hat)\n",
    "        return F.log_softmax(x, dim=1), F.log_softmax(y_hat,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLPAConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A GCN-LPA layer. Please refer to: https://arxiv.org/abs/2002.06755\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, adj, bias=True):\n",
    "        super(GCNLPAConv, self).__init__()\n",
    "#         Creating features from given data\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "#         Reset for LPA so unlabeled nodes do not overpowered labeled\n",
    "        self.reset_parameters()\n",
    "        self.adjacency_mask = Parameter(adj.clone()).to_dense()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, adj, y):\n",
    "        #Creating adjacency matrix\n",
    "        adj = adj.to_dense()\n",
    "        # W * x\n",
    "        support = torch.mm(x, self.weight)\n",
    "        \n",
    "        # Row-Normalize: D^-1 * (A') to normalize edge weights\n",
    "        adj = adj * self.adjacency_mask\n",
    "        adj = F.normalize(adj, p=1, dim=1) \n",
    "\n",
    "        # output = D^-1 * A' * X * W\n",
    "        output = torch.mm(adj, support)\n",
    "        # y' = D^-1 * A' * y ; matrix multiplcation to propagate labels to neighbors\n",
    "        y_hat = torch.mm(adj, y)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias, y_hat\n",
    "        else:\n",
    "            return output, y_hat\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
